{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries to be installed\n",
    "\n",
    "# !pip install transformers\n",
    "# !pip install -U datasets\n",
    "# !pip install soundfile\n",
    "# !pip install librosa\n",
    "# !pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"librispeech_asr\",\n",
    "                       split=\"train.clean.100\",\n",
    "                       streaming=True,\n",
    "                       trust_remote_code=True)\n",
    "# to load in interable dataset, streaming=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_head = dataset.take(5)\n",
    "list(dataset_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dataset_head)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio as IPythonAudio\n",
    "\n",
    "IPythonAudio(example[\"audio\"][\"array\"],\n",
    "             rate=example[\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic speech recognition pipeline\n",
    "asr = pipeline(task=\"automatic-speech-recognition\",\n",
    "               model=\"./models/distil-whisper/distil-small.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr.feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example['audio']['sampling_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr(example[\"audio\"][\"array\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.Blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_speech(filepath):\n",
    "    if filepath is None:\n",
    "        gr.Warning(\"No audio found, please retry.\")\n",
    "        return \"\"\n",
    "    output = asr(filepath)\n",
    "    return output[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_transcribe = gr.Interface(\n",
    "    fn=transcribe_speech,\n",
    "    inputs=gr.Audio(sources=\"microphone\",\n",
    "                    type=\"filepath\"),\n",
    "    outputs=gr.Textbox(label=\"Transcription\",\n",
    "                       lines=3),\n",
    "    allow_flagging=\"never\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_transcribe = gr.Interface(\n",
    "    fn=transcribe_speech,\n",
    "    inputs=gr.Audio(sources=\"upload\",\n",
    "                    type=\"filepath\"),\n",
    "    outputs=gr.Textbox(label=\"Transcription\",\n",
    "                       lines=3),\n",
    "    allow_flagging=\"never\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with demo:\n",
    "    gr.TabbedInterface(\n",
    "        [mic_transcribe,\n",
    "         file_transcribe],\n",
    "        [\"Transcribe Microphone\",\n",
    "         \"Transcribe Audio File\"],\n",
    "    )\n",
    "\n",
    "demo.launch(share=True, \n",
    "            server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For longer audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio,sampling_rate=sf.read('narration_example.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr.feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asr(audio)\n",
    "\n",
    "# could throw error due to multichannel audio input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the audio from stereo to mono (Using librosa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "audio_transposed = np.transpose(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_transposed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_mono = librosa.to_mono(audio_transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPythonAudio(audio_mono,rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asr(audio_mono)\n",
    "\n",
    "# the above may give wrong output due to difference in sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr.feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_16KHz = librosa.resample(audio_mono,\n",
    "                               orig_sr=sampling_rate,\n",
    "                               target_sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr(\n",
    "    audio_16KHz,\n",
    "    chunk_length_s=30, # 30 seconds\n",
    "    batch_size=4,\n",
    "    return_timestamps=True,\n",
    ")[\"chunks\"]\n",
    "\n",
    "# processing in batches\n",
    "# renning each batch simultaneouly means theat number of models together\n",
    "# theis will also use some overlapping and then connect the final result\n",
    "# the results will be chunks of audio with timestanp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "demo = gr.Blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_long_form(filepath):\n",
    "    if filepath is None:\n",
    "        gr.Warning(\"No audio found, please retry.\")\n",
    "        return \"\"\n",
    "    output = asr(\n",
    "      filepath,\n",
    "      max_new_tokens=256,\n",
    "      chunk_length_s=30,\n",
    "      batch_size=8,\n",
    "    )\n",
    "    return output[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_transcribe = gr.Interface(\n",
    "    fn=transcribe_long_form,\n",
    "    inputs=gr.Audio(sources=\"microphone\",\n",
    "                    type=\"filepath\"),\n",
    "    outputs=gr.Textbox(label=\"Transcription\",\n",
    "                       lines=3),\n",
    "    allow_flagging=\"never\")\n",
    "\n",
    "file_transcribe = gr.Interface(\n",
    "    fn=transcribe_long_form,\n",
    "    inputs=gr.Audio(sources=\"upload\",\n",
    "                    type=\"filepath\"),\n",
    "    outputs=gr.Textbox(label=\"Transcription\",\n",
    "                       lines=3),\n",
    "    allow_flagging=\"never\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with demo:\n",
    "    gr.TabbedInterface(\n",
    "        [mic_transcribe,\n",
    "         file_transcribe],\n",
    "        [\"Transcribe Microphone\",\n",
    "         \"Transcribe Audio File\"],\n",
    "    )\n",
    "demo.launch(share=True, \n",
    "            server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
